#!/bin/bash --login
########## SBATCH Lines for Resource Request ##########
 
#SBATCH --time=02:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --nodes=1-5                 # number of different nodes - could be an exact number or a range of nodes (same as -N)
#SBATCH --ntasks=1                  # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=2           # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem-per-cpu=5G            # memory required per allocated CPU (or core) - amount of memory (in bytes)
#SBATCH --job-name ctcf      # you can give your job a name for easier identification (same as -J)
 
########## Command Lines to Run ##########
 
module purge
module load icc/2016.3.210-GCC-5.4.0-2.26
module load impi/5.1.3.181
module load BEDTools

#module load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g.
cd $SLURM_SUBMIT_DIR || exit                 ### change to the directory where your code is located

#### NOTES ####
# Everything in the final analysis is using HG37/HG19
# But some of the SNP catagories we're using ONLY exist for Hg38, so we are downloading both, 
# and doing a backwards liftover of some of the files after filtering
################# Only use ACC_neuron and ACC_glia


mkdir ConsByBase_annots
cd ConsByBase_annots || exit

python3 ../ubiquitous-succotash/1_preprocess/ConservationByBase.py

#slower than expected evolution
sed 's/, /\t/g' ConsByBase.csv | awk '{if($5>.5)print $1 "\t" $2 "\t" $3 "\t" $4 "\t" $5}' > ConsByBase.tsv

awk '{ print $4 }' ConsByBase.tsv > ConsByBase_LDSR.txt

scontrol show job $SLURM_JOB_ID 
