#!/bin/bash --login
########## SBATCH Lines for Resource Request ##########
 
#SBATCH --time=02:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --nodes=1-5                 # number of different nodes - could be an exact number or a range of nodes (same as -N)
#SBATCH --ntasks=1                  # number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=2           # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem-per-cpu=30G            # memory required per allocated CPU (or core) - amount of memory (in bytes)
#SBATCH --job-name original      # you can give your job a name for easier identification (same as -J)
 
########## Command Lines to Run ##########
 
module purge

#module load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g.
module load icc/2016.3.210-GCC-5.4.0-2.26
module load impi/5.1.3.181
module load BEDTools

cd $SLURM_SUBMIT_DIR || exit 

##### NOTES #####
# These SNPs were already classified by Finucane: https://www.nature.com/articles/s41588-018-0081-4
# We just need to pull them back out into individual SNP files
# Run from main directory
#################


# First, make a list of features to search for in the Ensembl genome gff, then loop through searching for them

# 6 Coding_UCSC
# 8 Conserved_LindbladToh
# 11 CTCF_Hoffman.extend.500.bed
# 36 Intron_UCSC.bed
# 38 PromoterFlanking_Hoffman.bed
# 40 Promoter_UCSC.bed
# 50 TSS_Hoffman.bed
# 52 UTR_3_UCSC.bed
# 54 UTR_5_UCSC.bed

mkdir original_annots
cd original_annots || exit

rm *_LDSR.txt
#for col in `seq 6 57` 
for col in 6 8 40 50 52 54 # removed 11, 36, 38
    do for baseline in `seq 1 22`
        do NAME=`zcat ../RawData/baseline_v1.1/baseline.${baseline}.annot.gz | head -1 | cut -f ${col}`
        zcat ../RawData/baseline_v1.1/baseline.${baseline}.annot.gz | cut -f 3,${col} | awk '{if($2==1)print $1}' >> ${NAME}_LDSR.txt
    done 
done
        

scontrol show job $SLURM_JOB_ID 
